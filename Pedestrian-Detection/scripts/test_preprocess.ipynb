{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(xml_files, output_file_path):\n",
    "    cumulative_images = []\n",
    "    cumulative_annotations = []\n",
    "    cumulative_categories = [{\"id\": 1, \"name\": \"person\"}]  # Static category\n",
    "\n",
    "    image_id_counter = 1\n",
    "    annotation_id_counter = 1\n",
    "\n",
    "    for xml_file_path in xml_files:\n",
    "        images, annotations = parse_xml_for_person(xml_file_path)\n",
    "\n",
    "        # Adjust image and annotation IDs to be cumulative and unique\n",
    "        for img in images:\n",
    "            img['id'] = image_id_counter\n",
    "            cumulative_images.append(img)\n",
    "            image_id_counter += 1\n",
    "\n",
    "        for ann in annotations:\n",
    "            ann['id'] = annotation_id_counter\n",
    "            ann['image_id'] = ann['image_id'] + image_id_counter - len(images) - 1  # Adjusting image_id\n",
    "            cumulative_annotations.append(ann)\n",
    "            annotation_id_counter += 1\n",
    "\n",
    "        print(f'Processed {xml_file_path}')\n",
    "\n",
    "    # Combine everything into the COCO format\n",
    "    coco_format_data = {\n",
    "        \"images\": cumulative_images,\n",
    "        \"annotations\": cumulative_annotations,\n",
    "        \"categories\": cumulative_categories\n",
    "    }\n",
    "\n",
    "    # Save the combined data to a single JSON file\n",
    "    with open(output_file_path, 'w') as json_file:\n",
    "        json.dump(coco_format_data, json_file, indent=4)\n",
    "\n",
    "    print(f'Combined annotations saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset path\n",
    "dataset_base_folder = '/content/drive/My Drive/dataset'  # Path to the base folder containing subfolders with XML files\n",
    "\n",
    "# Get list of all XML files\n",
    "xml_files = []\n",
    "for subdir, _, files in os.walk(dataset_base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            xml_file_path = os.path.join(subdir, file)\n",
    "            xml_files.append(xml_file_path)\n",
    "\n",
    "# Split the dataset into train and val with 80:20 ratio\n",
    "train_files, val_files = train_test_split(xml_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Process and save train dataset\n",
    "train_output_path = '/content/drive/My Drive/dataset/train_annotations.json'\n",
    "process_and_save(train_files, train_output_path)\n",
    "\n",
    "# Process and save val dataset\n",
    "val_output_path = '/content/drive/My Drive/dataset/val_annotations.json'\n",
    "process_and_save(val_files, val_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
