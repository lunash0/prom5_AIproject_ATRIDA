{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import time\n",
    "import torch\n",
    "import tqdm \n",
    "from custom_utils import * \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "video_name = 'after_detection'\n",
    "input_video = '/home/yoojinoh/Others/PR/PedDetect-Data/2954065-hd_1920_1080_30fps.mp4'\n",
    "output_video = f'/home/yoojinoh/Others/PR/PedDetect-Data/{video_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model\n",
    "\n",
    "def load_model(checkpoint_path, num_classes, device):\n",
    "    model = build_model(num_classes)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval() # Eval mode\n",
    "\n",
    "    return model\n",
    "\n",
    "def process_frame(frame, model, device, iou_thresh=0.3, confidence_threshold=0.5):\n",
    "    image = F.to_tensor(frame).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)[0]\n",
    "\n",
    "    output = apply_nms(output, iou_thresh)\n",
    "    output = filter_boxes_by_score(output, confidence_threshold)\n",
    "\n",
    "    boxes = output[\"boxes\"].cpu().numpy()\n",
    "    labels = output[\"labels\"].cpu().numpy()\n",
    "    scores = output[\"scores\"].cpu().numpy()\n",
    "\n",
    "    return boxes, labels, scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label(label):\n",
    "    if label == 1:\n",
    "            class_name = 'person'\n",
    "    else:\n",
    "        class_name = ''\n",
    "    return class_name\n",
    "\n",
    "def draw_boxes_on_frame(frame, boxes, labels, scores, thr = 0.5, save_path = None):\n",
    "#    image = cv2.imread(image_path) \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        print(score)\n",
    "        if score >= thr:\n",
    "            x1, y1, x2, y2 = map(int, box) # box.astype(int)\n",
    "            class_name = check_label(label)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f'{class_name} : {score:.2f}', (x1, y1- 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_frame(model, img_frame, confidence_threshold, score_threshold):\n",
    "    boxes, labels, scores = process_frame(img_frame, model, device, confidence_threshold=confidence_threshold)\n",
    "    img_frame = draw_boxes_on_frame(img_frame, boxes, labels, scores, threshold=score_threshold)\n",
    "    return img_frame\n",
    "\n",
    "def detect_video(model, input_path, output_path, confidence_threshold=0.5, score_threshold=0.5):\n",
    "    # Set VideoCapture, VideoWriter\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID') # format of video writer\n",
    "    video_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))) \n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    video_writer = cv2.VideoWriter(output_video, codec, video_fps, video_size)\n",
    "    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f'Total number of frame : {frame_cnt}')\n",
    "\n",
    "    while True:\n",
    "        hasFrame, img_frame = cap.read()\n",
    "        if not hasFrame:\n",
    "            print(f'Processed all frames')\n",
    "            break \n",
    "            \n",
    "        img_frame = detect_frame(model, img_frame, confidence_threshold, score_threshold)\n",
    "        video_writer.write(img_frame)\n",
    "    \n",
    "    video_writer.realse()\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoojinoh/.miniconda3/envs/deeplearning/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yoojinoh/.miniconda3/envs/deeplearning/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frame : 884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@973.383] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.10.0) /io/opencv/modules/videoio/src/cap_images.cpp:430: error: (-215:Assertion failed) !filename_pattern.empty() in function 'open'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'Tensor' and 'torch.device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_path, num_classes, device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdetect_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mdetect_video\u001b[0;34m(model, input_path, output_path, confidence_threshold, score_threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed all frames\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m \n\u001b[0;32m---> 23\u001b[0m     img_frame \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     video_writer\u001b[38;5;241m.\u001b[39mwrite(img_frame)\n\u001b[1;32m     26\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mrealse()\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mdetect_frame\u001b[0;34m(model, img_frame, confidence_threshold, score_threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_frame\u001b[39m(model, img_frame, confidence_threshold, score_threshold):\n\u001b[0;32m----> 2\u001b[0m     boxes, labels, scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     img_frame \u001b[38;5;241m=\u001b[39m draw_boxes_on_frame(img_frame, boxes, labels, scores, threshold\u001b[38;5;241m=\u001b[39mscore_threshold)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_frame\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mprocess_frame\u001b[0;34m(frame, model, device, iou_thresh, confidence_threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(image)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m apply_nms(output, iou_thresh)\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_boxes_by_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m boxes \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     26\u001b[0m labels \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Others/PR/ATRIDA_prom5_AIproject/Pedestrian-Detection/custom_utils.py:68\u001b[0m, in \u001b[0;36mfilter_boxes_by_score\u001b[0;34m(output, threshold)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_boxes_by_score\u001b[39m(output, threshold):\n\u001b[0;32m---> 68\u001b[0m     keep \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\n\u001b[1;32m     69\u001b[0m     filtered_output \u001b[38;5;241m=\u001b[39m {k: v[keep] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_output\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'Tensor' and 'torch.device'"
     ]
    }
   ],
   "source": [
    "model_path = '/home/yoojinoh/Others/PR/ATRIDA_prom5_AIproject/Pedestrian-Detection/outputs/best_fasterrcnn_e6s0.7686803706057437l0.10222071961704958.pth'\n",
    "num_classes = 2 \n",
    "\n",
    "model = load_model(model_path, num_classes, device)\n",
    "detect_video(model, input_video, output_video, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
